<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.119.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Speakers & Sessions &#183; SAMPTA 2023</title><meta name=description content><link type=text/css rel=stylesheet href=https://sampta2023.github.io/css/print.css media=print><link type=text/css rel=stylesheet href=https://sampta2023.github.io/css/poole.css><link type=text/css rel=stylesheet href=https://sampta2023.github.io/css/syntax.css><link type=text/css rel=stylesheet href=https://sampta2023.github.io/css/hyde.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png></head><body class=theme-base-08><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://sampta2023.github.io/><h1>SAMPTA 2023</h1></a><p class=lead>Sampling Theory and Applications Conference. July 10-14 at Yale.</p></div><nav><ul class=sidebar-nav><li><a href=https://sampta2023.github.io/>Home</a></li><li><a href=https://web.cvent.com/event/903a729f-b298-40ca-a060-3f0447be7bbc/summary>Conference Registration</a></li><li><a href=/about/>About</a></li><li><a href=/accepted-paper-talks/>Accepted Papers and Abstracts</a></li><li><a href=/call-for-papers/>Call for Papers</a></li><li><a href=/dinner/>Dinner</a></li><li><a href=/excursions/>Excursions</a></li><li><a href=/schedule/>Schedule</a></li><li><a href=/speakers/>Speakers & Sessions</a></li><li><a href=/venue/>Venue</a></li><li><a href="https://openreview.net/group?id=SampTA/2023/Conference">Submit your paper</a></li></ul></nav><p>&copy; 2023. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Speakers & Sessions</h1><time datetime=2023-01-08T00:00:00Z class=post-date>Sun, Jan 8, 2023</time>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><h1 id=plenary-speakers>Plenary Speakers</h1><p>Our plenary speakers are:</p><ul><li><a href=https://www.cs.ox.ac.uk/people/michael.bronstein/>Michael Bronstein</a> (Oxford)</li><li><a href=https://www.ai.math.uni-muenchen.de/members/professor/kutyniok/index.html>Gitta Kutyniok</a> (LMU)</li><li><a href=http://cs.yale.edu/homes/spielman/>Dan Spielman</a> (Yale)</li><li><a href=https://www.ams.jhu.edu/villar/>Soledad Villar</a> (Johns Hopkins)</li><li><a href=http://www.columbia.edu/~skk2175/>Samory Kpotufe</a> (Columbia University)</li><li><a href=https://users.math.msu.edu/users/iwenmark/>Mark Iwen</a> (Michigan State University)</li><li><a href=http://www.mathc.rwth-aachen.de/~rauhut/home/>Holger Rauhut</a> (Aachen University)</li><li><a href=https://www.maths.ox.ac.uk/people/coralia.cartis>Coralia Cartis</a> (Oxford)</li><li><a href=https://willett.psd.uchicago.edu>Rebecca Willett</a> (University of Chicago)</li></ul><h1 id=special-sessions>Special Sessions</h1><p>Our special sessions will bring together domain experts to present on topics including <em>Manifold Learning from Data</em>, <em>Graph Signal Processing</em>, <em>Topological Data Analysis</em>, and <em>Sampling Theory for Neural Activity Data</em>, among others. Each session will feature 3-4 speakers, and run for 2 hours.</p><h2 id=manifold-learning-from-data>Manifold Learning from Data</h2><p>Hosted by <a href=https://www.usu.edu/math/directory/faculty/moon-kevin>Kevin Moon</a> (Utah State).</p><p>Manifold learning algorithms are powerful data analysis methods that are based on the manifold assumption. Informally, the manifold assumption states that high dimensional data are approximated well by a small number of locally smooth dimensions. This special session focuses on recent manifold learning algorithms or recent mathematical insights obtained about existing manifold learning algorithms. Applications of interest include, but are not limited to, dimensionality reduction, data denoising, data visualization, data integration, density estimation, clustering, and semi-supervised learning.</p><p>Featuring:</p><ul><li><a href=https://cse.umn.edu/math/jeffrey-calder>Jeff Calder</a> (University of Minnesota)</li><li><a href=https://cpsc.yale.edu/people/ronald-coifman>Ronald Coifman</a> (Yale)</li><li><a href=https://roy.lederman.name/>Roy Lederman</a> (Yale)</li><li><a href=https://www.bell-labs.com/about/researcher-profiles/dankushnir/>Dan Kushnir</a> (Nokia Bell Labs)</li><li>Jake Rhodes (Idaho State University)</li></ul><h2 id=signal-and-image-processing>Signal and Image Processing</h2><p>Hosted by <a href=http://longxiuhuang.com/>Longxiu Huang</a> (Michigan State University) and <a href=https://web.math.ucsb.edu/~suitang/>Sui Tang</a> (UCSB).</p><p>This symposium will delve into the realm of graph-based signal and image processing. Experts from this field will come together to share their latest research findings, discuss current challenges and opportunities, and network with their peers. The event will cover a range of topics including signal and image processing techniques, sampling and reconstruction methods, graph neural networks, and their applications. With a focus on both theory and practical applications, this symposium promises to provide valuable insights and knowledge for those interested in these cutting-edge and rapidly growing areas.</p><p>This session will feature:</p><ul><li><a href=https://www.egr.msu.edu/people/profile/aviyente>Selin Aviyente</a> (Michigan State University): <strong>Low-rank and Smooth Tensor Recovery on Cartesian Product Graphs</strong></li><li><a href=https://ece.osu.edu/people/lee.8763>Kiryung Lee</a> (Ohio State): <strong>Stability Analysis of Resolving Pulses of Unknown Shape from Compressive Fourier Measurements</strong></li><li><a href=http://mishne.ucsd.edu/>Gal Mishne</a> (UCSD): <strong>Graph Laplacian Learning with Exponential Family Noise</strong></li><li><a href=https://math.as.uky.edu/users/jqi229>Jing Qin</a> (University of Kentucky): <strong>Fast Dual-Graph Regularized Background Foreground Separation</strong></li></ul><h2 id=methods-for-low-rank-matrices-and-tensors>Methods for Low Rank Matrices and Tensors</h2><p>Hosted by <a href=https://dominiksto.github.io/>Dominik Stöger</a> (KU Eichstätt-Ingolstadt) and <a href=https://www.math.uci.edu/node/36395>Anna Ma</a> (University of California Irvine),</p><p>Featuring:</p><ul><li><a href=https://www.lijunding.net>Lijun Ding</a> (UW Madison)</li><li><a href=https://larakassab.weebly.com>Lara Kassab</a> (UCLA)</li><li><a href=https://palinasalanevich.wordpress.com/>Palina Salanevich</a> (University of Utrecht)</li><li><a href=https://sites.google.com/uci.edu/yizhezhu>Yizhe Zhu</a> (UC Irvine)</li></ul><h2 id=data-geometry-and-optimization>Data Geometry and Optimization</h2><p>Hosted by <a href="https://sites.google.com/umich.edu/rsonthal?pli=1">Rishi Sonthalia</a> (UCLA) and <a href=https://sites.google.com/view/perlmutma/home>Michael Perlmutter</a> (Boise State University).</p><p>Prior work in optimization has focused on the function that is being optimized. However, in recent years, there has been a shift towards looking at the geometry of the data as well. By appropriately incorporating the geometry of the data, we can get better convergence, change the local optima that we converge to, or better understand the properties of the optima that we converge to. In this mini-symposia, we invite speakers to talk about recent advances in optimization that incorporate the geometry of the data to solve or understand the optimization problem.</p><p>This sessions will feature:</p><ul><li><a href=https://www.math.ucla.edu/~montufar/>Guido Montufar</a> (UCLA)</li><li><a href=https://stat.wisc.edu/staff/trillos-nicolas-garcia/>Nicholas Garcia Trillos</a> (UW Madison)</li><li><a href=http://melanie-weber.com>Melanie Weber</a> (Harvard)</li></ul><h2 id=randomized-algorithms-for-complex-data>Randomized Algorithms for Complex Data</h2><p>Hosted by <a href=https://erebrova.github.io/>Elizaveta Rebrova</a> (Princeton).</p><p>Over recent years, randomized methods led to breakthroughs in many areas, including large-scale optimization, numerical linear algebra, scientific computing, and machine learning. However, the interest in developing efficient, interpretable, and theory-supported randomized algorithms is only growing, partially motivated by the emergence of large-scale information-rich data — such as high-dimensional, multi-modal (tensor), or graph data. This SampTA section brings together the researchers creating and analyzing modern randomized algorithms for diverse problems involving large-scale complex data.</p><p>Featuring:</p><ul><li><a href=https://chen.pw>Tyler Chen</a> (NYU)</li><li><a href=http://www.columbia.edu/~rd2714/>Rishabh Dudeja</a> (Harvard)</li><li><a href=https://engineering.jhu.edu/faculty/nicolas-loizou/>Nicolas Loizou</a> (Hopkins)</li><li><a href=https://math.wisc.edu/staff/jin-ruhui/>Ruhui Jin</a> (UW Madison)</li></ul><h2 id=machine-learning-and-signal-processing-on-graphs-and-manifolds>Machine Learning and Signal Processing on Graphs and Manifolds</h2><p>Hosted by <a href=https://sites.google.com/view/perlmutma/home>Michael Perlmutter</a> (Boise State University).</p><p>Recent years have seen rapid growth in the development of data science and machine learning techniques for graph- and manifold-structured data. These techniques typically aim to find and utilize the (often hidden) intrinsic geometric structure of the data set. This session will feature several examples of such techniques which are based on methods initially developed in the context of graph signal processing.</p><p>This session will feature <a href=https://joycechew.github.io/>Joyce Chew</a> (UCLA), <a href=https://www.math.ucdavis.edu/~saito/>Naoki Saito</a> (UC Davis), <a href=https://www.anna-little.com>Anna Little</a> (University of Utah), and <a href=https://sites.google.com/view/luana-ruiz/home>Luana Ruiz</a> (Johns Hopkins).</p><h2 id=sampling-theory-in-neuroscience>Sampling Theory in Neuroscience</h2><p>Hosted by <a href=https://bastian.rieck.me>Bastian Rieck</a> (Helmholtz Munich, Germany) and <a href=https://www.guillaumelajoie.com/>Guillaume Lajoie</a> (University of Montreal).</p><p>Featuring:</p><ul><li>Scaling behavior in big-data neuroscience towards precision medicine, <a href=https://www.mcgill.ca/bbme/danilo-bzdok>Danilo Bzdok</a> (McGill)</li><li>Half-Hop: A graph upsampling approach for improved message passing, <a href=https://bme.gatech.edu/bme/faculty/Eva-Dyer>Eva Dyer</a> (Georgia Tech)</li><li>Quantifying and comparing neural manifold structure with limited samples, <a href=http://neurostatslab.org>Alex Williams</a> (NYU & Flatiron Institute)</li><li>Modeling brain-wide neural recordings to extract behaviorally-relevant structure, <a href=https://ma.ttperi.ch/>Matthew Perich</a> (UdeM)</li><li>Meta-Learning to leverage population data for personalized neurostimulation optimization, <a href=https://www.guillaumelajoie.com/>Guillaume Lajoie</a> (UdeM)</li></ul><h2 id=quantization-in-signal-processing-and-data-science>Quantization in Signal Processing and Data Science</h2><p>Hosted by <a href=https://johannes-maly.github.io/>Johannes Maly</a> (LMU Munich) and <a href=https://www.researchgate.net/profile/Sjoerd-Dirksen>Sjoerd Dirksen</a> (Ultrecht University)</p><p>A common challenge in signal processing and data science is that digital processing requires to quantize all involved quantities to a finite alphabet. Whereas many numerical methods with robust guarantees remain reliable under such perturbations if the quantization alphabet is sufficiently large, in modern large-scale applications it is often necessary to apply extremely coarse quantization. This setting requires tailored algorithms and novel mathematical methods to analyze their performance. In this special session we will bring together experts from applied mathematics and engineering to discuss recent advances in the theory of quantization.</p><p>This session will feature four invited talks:</p><ol><li>Near-optimality of \(\Sigma\Delta\) quantization for \(L^2\)-approximation with polynomials in Bernstein form (<a href=https://www.math.nyu.edu/people/profiles/GUNTURK_Sinan.html>Sinan Gunturk</a>, Courant Institute)</li><li>Learning Signal Spaces from Incomplete Binary Measurements (<a href=https://laurentjacques.gitlab.io>Laurent Jacques</a>, UC Louvain)</li><li>Algorithms and Theory for Quantizing Neural Networks (<a href=https://mathweb.ucsd.edu/~rsaab/>Rayan Saab</a>, UCSD)</li><li>Digital Halftoning via Mixed-Order Weighted Sigma-Delta Modulation (<a href=https://annaveselovska.com/>Hanna Veselovska</a>, Technical University of Munich)</li></ol><h2 id=event-driven-sampling-time-encoding-and-sampling-with-integral-transforms>Event-driven sampling, time-encoding and sampling with integral transforms</h2><p>Hosted by <a href=https://sites.google.com/site/jlromeroresearch/>Jose Luis Romero</a> (University of Vienna) and <a href=https://sites.google.com/view/dianacarbajal/home>Diana Carbajal</a> (University of Vienna).</p><p>The focal point of the session is event-driven sampling and time-encoding machines. These sampling schemes are motivated by the need for energy efficiency and rely on capturing the times where significant events of a signal occur, rather than following a clock-based pattern. Sampling events are often modeled by level sets of certain integral transforms and lead to non-uniform sampling geometries. The session also seeks to highlight technical parallelisms between event-driven sampling on the one hand, and the theory of non-uniform sampling supplemented with derivatives and sampling strategies adapted to varying bandwidth, on the other.</p><p>This special session will feature five invited talks:</p><ul><li>Pseudo-inversion of integration-based time encoding using POCS (<a href=https://ieeexplore.ieee.org/author/37269394400>Nguyen T. Thao</a>, <a href=https://deepai.org/profile/dominik-rzepka>Dominik Rzepka</a>, Marek Miskowicz)</li><li>Iterative reconstruction of bandlimited signals from nonuniform samples by sliding periodization of nonuniformity (Nguyen T. Thao, Dominik Rzepka, Marek Miskowicz)</li><li>Model-Driven Quantization for Time Encoding Machines (<a href=https://www.imperial.ac.uk/people/d.florescu>Dorian Florescu</a>)</li><li>Sampling theorems with derivatives in shift-invariant spaces generated<br>by exponential B-splines (<a href=https://homepage.univie.ac.at/irina.shafkulovska/>Irina Shafkulovska</a>, <a href=https://homepage.univie.ac.at/karlheinz.groechenig/>Karlheinz Gröchenig</a>)</li><li>Sampling theorems in spaces of variable bandwidth generated via Wilson basis (<a href=https://homepage.univie.ac.at/beatrice.andreolli/>Beatrice Andreolli</a>, Karlheinz Gröchenig)</li></ul></div></main></body></html>